{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AI-Based Crop Health Monitoring Using Drone Multispectral Data\n## Comprehensive Capstone Project - Robust Final Analysis\n\n**Author:** Capstone Submission | **Date:** January 2026\n\n### Project Overview\nThis notebook presents an end-to-end AI pipeline for detecting crop stress using multispectral vegetation indices derived from drone imagery.\n\n**Objectives:**\n1. Understand vegetation indices and their role in crop health assessment\n2. Develop and compare multiple machine learning classification models\n3. Generate spatial stress maps with grid-level aggregation\n4. Provide actionable drone inspection strategies\n5. Reflect on limitations and propose improvements\n\n**Dataset:** 1200 spatial observations with 13 vegetation index features + spatial coordinates\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Environment Setup & Data Loading"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core Data Science Libraries\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle\ntry:\n    import plotly.express as px\n    import plotly.graph_objects as go\n    PLOTLY_AVAILABLE = True\nexcept:\n    PLOTLY_AVAILABLE = False\n    print(\"Plotly not available - will use matplotlib only\")\n\n# Statistical Analysis\nfrom scipy.stats import mannwhitneyu, chi2_contingency\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Machine Learning - Models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Machine Learning - Preprocessing & Evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n    precision_recall_curve, f1_score, accuracy_score, precision_score, recall_score\n)\nfrom sklearn.inspection import permutation_importance\n\n# Imbalanced Learning\ntry:\n    from imblearn.over_sampling import SMOTE\n    SMOTE_AVAILABLE = True\nexcept:\n    SMOTE_AVAILABLE = False\n    print(\"imbalanced-learn not available - will skip SMOTE\")\n\n# Explainability (optional - may not be installed)\ntry:\n    import shap\n    SHAP_AVAILABLE = True\nexcept:\n    SHAP_AVAILABLE = False\n    print(\"SHAP not available - will skip SHAP analysis\")\n\ntry:\n    import lime\n    import lime.lime_tabular\n    LIME_AVAILABLE = True\nexcept:\n    LIME_AVAILABLE = False\n    print(\"LIME not available - will skip LIME analysis\")\n\n# Visualization Settings\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 10\n\n# Reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"=\"*80)\nprint(\"ENVIRONMENT SETUP COMPLETE\")\nprint(\"=\"*80)\nprint(f\"\u2713 Random seed: {RANDOM_STATE}\")\nprint(f\"\u2713 SMOTE available: {SMOTE_AVAILABLE}\")\nprint(f\"\u2713 SHAP available: {SHAP_AVAILABLE}\")\nprint(f\"\u2713 LIME available: {LIME_AVAILABLE}\")\nprint(f\"\u2713 Plotly available: {PLOTLY_AVAILABLE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 Load Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the crop health dataset\nDATA_URL = \"https://docs.google.com/spreadsheets/d/1wPL7_G65NBY7801PfKhbsM7ujANoID6DIzb2zmcJ1yM/export?format=csv\"\n\ntry:\n    df = pd.read_csv(DATA_URL)\n    print(\"\u2713 Dataset loaded successfully!\")\n    print(f\"\\nShape: {df.shape}\")\n    print(f\"Features: {df.shape[1]}\")\n    print(f\"Samples: {df.shape[0]}\")\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    print(\"Please check the URL or internet connection\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Task 1: Data Understanding & Exploration\n\n### 2.1 Initial Data Inspection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display first rows\nprint(\"\\n\" + \"=\"*80)\nprint(\"FIRST 5 ROWS\")\nprint(\"=\"*80)\ndisplay(df.head())\n\n# Dataset information\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATASET INFORMATION\")\nprint(\"=\"*80)\ndf.info()\n\n# Statistical summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"STATISTICAL SUMMARY\")\nprint(\"=\"*80)\ndisplay(df.describe())\n\n# Check for missing values\nprint(\"\\n\" + \"=\"*80)\nprint(\"MISSING VALUES\")\nprint(\"=\"*80)\nmissing = df.isnull().sum()\nif missing.sum() == 0:\n    print(\"\u2713 No missing values detected\")\nelse:\n    print(missing[missing > 0])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 Understanding Vegetation Indices\n\n| **Index** | **Purpose** | **Healthy Range** |\n|-----------|-------------|-------------------|\n| **NDVI** | Vegetation vigor & biomass | 0.6 - 0.9 |\n| **GNDVI** | Chlorophyll content | 0.5 - 0.8 |\n| **SAVI** | Soil-adjusted vegetation | 0.5 - 0.8 |\n| **EVI** | Enhanced vegetation index | 0.4 - 0.7 |\n| **Red Edge** | Early stress detection | Higher = healthier |\n| **NIR Reflectance** | Plant cell structure | 0.4 - 0.6 |\n| **Canopy Density** | Vegetation coverage | 0.7 - 0.95 |\n| **Moisture Index** | Water stress indicator | 0.6 - 0.9 |\n\n**Stressed crops typically show:**\n- Lower NDVI, GNDVI, SAVI, EVI\n- Reduced red-edge reflectance\n- Lower NIR reflectance (damaged cells)\n- Reduced canopy density\n- Lower moisture index"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Target Variable Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze class distribution\nprint(\"\\n\" + \"=\"*80)\nprint(\"CLASS DISTRIBUTION\")\nprint(\"=\"*80)\n\ntarget_counts = df['crop_health_label'].value_counts()\ntarget_pct = (target_counts / len(df) * 100).round(2)\n\nprint(f\"\\nHealthy: {target_counts.get('Healthy', 0)} ({target_pct.get('Healthy', 0)}%)\")\nprint(f\"Stressed: {target_counts.get('Stressed', 0)} ({target_pct.get('Stressed', 0)}%)\")\n\n# Calculate imbalance ratio\nif len(target_counts) > 1:\n    imbalance_ratio = target_counts.max() / target_counts.min()\n    print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n    if imbalance_ratio > 1.5:\n        print(\"\u26a0\ufe0f  Dataset is imbalanced - will apply SMOTE\")\n    else:\n        print(\"\u2713 Dataset is relatively balanced\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar plot\ntarget_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'], alpha=0.8)\naxes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Crop Health Status')\naxes[0].set_ylabel('Count')\naxes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\naxes[0].grid(axis='y', alpha=0.3)\n\n# Add value labels\nfor i, v in enumerate(target_counts):\n    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n\n# Pie chart\ncolors = ['#2ecc71', '#e74c3c']\naxes[1].pie(target_counts, labels=target_counts.index, autopct='%1.1f%%',\n            startangle=90, colors=colors, explode=(0.05, 0.05), shadow=True)\naxes[1].set_title('Class Distribution (%)', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.4 Exploratory Data Analysis (EDA)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distribution of key vegetation indices by health status\nprint(\"\\n\" + \"=\"*80)\nprint(\"VEGETATION INDICES BY HEALTH STATUS\")\nprint(\"=\"*80)\n\nveg_indices = ['ndvi_mean', 'gndvi', 'savi', 'evi', 'moisture_index', 'canopy_density']\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.ravel()\n\nfor idx, col in enumerate(veg_indices):\n    healthy_data = df[df['crop_health_label'] == 'Healthy'][col].dropna()\n    stressed_data = df[df['crop_health_label'] == 'Stressed'][col].dropna()\n    \n    # Violin plot\n    parts = axes[idx].violinplot(\n        [healthy_data, stressed_data],\n        positions=[0, 1],\n        showmeans=True,\n        showmedians=True\n    )\n    \n    axes[idx].set_title(col.upper(), fontsize=12, fontweight='bold')\n    axes[idx].set_xticks([0, 1])\n    axes[idx].set_xticklabels(['Healthy', 'Stressed'])\n    axes[idx].set_ylabel('Value')\n    axes[idx].grid(axis='y', alpha=0.3)\n\nplt.suptitle('Vegetation Indices Distribution by Crop Health', \n             fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Statistical significance testing\nprint(\"\\n\" + \"=\"*80)\nprint(\"STATISTICAL TESTS (Mann-Whitney U)\")\nprint(\"=\"*80)\nprint(\"H0: No difference between Healthy and Stressed\\n\")\n\nstat_results = []\nfor col in veg_indices:\n    healthy = df[df['crop_health_label'] == 'Healthy'][col].dropna()\n    stressed = df[df['crop_health_label'] == 'Stressed'][col].dropna()\n    \n    statistic, p_value = mannwhitneyu(healthy, stressed, alternative='two-sided')\n    \n    mean_diff = healthy.mean() - stressed.mean()\n    pooled_std = np.sqrt((healthy.std()**2 + stressed.std()**2) / 2)\n    cohens_d = mean_diff / pooled_std if pooled_std != 0 else 0\n    \n    stat_results.append({\n        'Feature': col,\n        'Healthy_Mean': healthy.mean(),\n        'Stressed_Mean': stressed.mean(),\n        'Mean_Diff': mean_diff,\n        'P-Value': p_value,\n        'Cohens_d': cohens_d,\n        'Significant': 'Yes' if p_value < 0.05 else 'No'\n    })\n\nstat_df = pd.DataFrame(stat_results).sort_values('P-Value')\ndisplay(stat_df)\n\nsig_count = sum(stat_df['Significant'] == 'Yes')\nprint(f\"\\n\u2713 Significant features (p < 0.05): {sig_count}/{len(stat_df)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.5 Correlation Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Correlation matrix\nprint(\"\\n\" + \"=\"*80)\nprint(\"CORRELATION ANALYSIS\")\nprint(\"=\"*80)\n\n# Select numerical features (exclude grid coordinates)\nnum_features = df.select_dtypes(include=[np.number]).columns.tolist()\nnum_features = [f for f in num_features if f not in ['grid_x', 'grid_y']]\n\ncorr_matrix = df[num_features].corr()\n\n# Plot\nplt.figure(figsize=(14, 12))\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\nsns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Correlation Matrix of Vegetation Indices', fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\n# Identify high correlations\nprint(\"\\nHighly Correlated Pairs (|r| > 0.8):\")\nhigh_corr = []\nfor i in range(len(corr_matrix)):\n    for j in range(i+1, len(corr_matrix)):\n        if abs(corr_matrix.iloc[i, j]) > 0.8:\n            high_corr.append({\n                'Feature_1': corr_matrix.columns[i],\n                'Feature_2': corr_matrix.columns[j],\n                'Correlation': corr_matrix.iloc[i, j]\n            })\n\nif high_corr:\n    display(pd.DataFrame(high_corr))\nelse:\n    print(\"\u2713 No highly correlated pairs found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.6 Spatial Pattern Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Spatial distribution\nprint(\"\\n\" + \"=\"*80)\nprint(\"SPATIAL DISTRIBUTION\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Scatter by health status\nfor label, color in [('Healthy', '#2ecc71'), ('Stressed', '#e74c3c')]:\n    mask = df['crop_health_label'] == label\n    axes[0].scatter(df[mask]['grid_x'], df[mask]['grid_y'],\n                   c=color, label=label, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n\naxes[0].set_xlabel('Grid X (East-West)', fontsize=12)\naxes[0].set_ylabel('Grid Y (North-South)', fontsize=12)\naxes[0].set_title('Spatial Distribution by Health Status', fontsize=14, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# NDVI heatmap\npivot_ndvi = df.pivot_table(index='grid_y', columns='grid_x', values='ndvi_mean', aggfunc='mean')\nim = axes[1].imshow(pivot_ndvi, cmap='RdYlGn', aspect='auto', interpolation='bilinear')\naxes[1].set_xlabel('Grid X', fontsize=12)\naxes[1].set_ylabel('Grid Y', fontsize=12)\naxes[1].set_title('NDVI Spatial Distribution', fontsize=14, fontweight='bold')\nplt.colorbar(im, ax=axes[1], label='NDVI Mean')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nGrid dimensions: {df['grid_x'].max()+1} x {df['grid_y'].max()+1}\")\nprint(f\"Total observations: {len(df)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Task 2: Machine Learning Model Development\n\n### 3.1 Data Preparation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare features and target\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATA PREPARATION\")\nprint(\"=\"*80)\n\n# Define features (exclude spatial coords and target)\nfeature_cols = [col for col in df.columns if col not in ['crop_health_label', 'grid_x', 'grid_y']]\nX = df[feature_cols].copy()\ny = df['crop_health_label'].copy()\n\n# Encode target\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nprint(f\"Feature matrix: {X.shape}\")\nprint(f\"Target vector: {y_encoded.shape}\")\nprint(f\"\\nClass encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\nprint(f\"\\nFeatures ({len(feature_cols)}):\")\nfor i, feat in enumerate(feature_cols, 1):\n    print(f\"  {i:2d}. {feat}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_encoded, test_size=0.2, random_state=RANDOM_STATE, stratify=y_encoded\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAIN-TEST SPLIT (80-20)\")\nprint(\"=\"*80)\nprint(f\"Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n\nprint(\"\\nTraining set distribution:\")\nfor class_idx, count in pd.Series(y_train).value_counts().sort_index().items():\n    print(f\"  {le.classes_[class_idx]}: {count} ({count/len(y_train)*100:.1f}%)\")\n\nprint(\"\\nTest set distribution:\")\nfor class_idx, count in pd.Series(y_test).value_counts().sort_index().items():\n    print(f\"  {le.classes_[class_idx]}: {count} ({count/len(y_test)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 Feature Selection & Scaling"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick feature importance check\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEATURE IMPORTANCE (Random Forest)\")\nprint(\"=\"*80)\n\nrf_temp = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\nrf_temp.fit(X_train, y_train)\n\nfeature_imp = pd.DataFrame({\n    'Feature': feature_cols,\n    'Importance': rf_temp.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"\\nTop 10 Features:\")\ndisplay(feature_imp.head(10))\n\n# Select top features (cumulative importance > 95%)\nfeature_imp['Cumulative'] = feature_imp['Importance'].cumsum()\nselected_features = feature_imp[feature_imp['Cumulative'] <= 0.95]['Feature'].tolist()\n\n# Ensure at least 8 features\nif len(selected_features) < 8:\n    selected_features = feature_imp.head(8)['Feature'].tolist()\n\nprint(f\"\\n\u2713 Selected {len(selected_features)} features\")\n\n# Visualize\nplt.figure(figsize=(12, 6))\nplt.barh(range(len(feature_imp)), feature_imp['Importance'], color='steelblue', alpha=0.8)\nplt.yticks(range(len(feature_imp)), feature_imp['Feature'])\nplt.xlabel('Importance')\nplt.title('Feature Importance Ranking', fontsize=14, fontweight='bold')\nplt.gca().invert_yaxis()\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply feature selection and scaling\nX_train_selected = X_train[selected_features].copy()\nX_test_selected = X_test[selected_features].copy()\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_selected)\nX_test_scaled = scaler.transform(X_test_selected)\n\n# Convert back to DataFrame\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train_selected.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test_selected.index)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEATURE SCALING\")\nprint(\"=\"*80)\nprint(f\"Original features: {X_train.shape[1]}\")\nprint(f\"Selected features: {X_train_scaled.shape[1]}\")\nprint(\"\u2713 StandardScaler applied (zero mean, unit variance)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Handle Class Imbalance (SMOTE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply SMOTE if needed\nprint(\"\\n\" + \"=\"*80)\nprint(\"CLASS IMBALANCE HANDLING\")\nprint(\"=\"*80)\n\nclass_counts = pd.Series(y_train).value_counts()\nimbalance_ratio = class_counts.max() / class_counts.min()\n\nif imbalance_ratio > 1.5 and SMOTE_AVAILABLE:\n    print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n    print(\"Applying SMOTE...\")\n    \n    smote = SMOTE(random_state=RANDOM_STATE)\n    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n    \n    print(f\"\\nBefore: {len(y_train)} samples\")\n    print(f\"After: {len(y_train_balanced)} samples\")\n    print(\"\u2713 Dataset balanced\")\nelse:\n    print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n    if not SMOTE_AVAILABLE:\n        print(\"\u26a0\ufe0f  SMOTE not available - proceeding without balancing\")\n    else:\n        print(\"\u2713 Dataset relatively balanced\")\n    X_train_balanced = X_train_scaled\n    y_train_balanced = y_train"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4 Model Training & Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train multiple models\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL TRAINING\")\nprint(\"=\"*80)\n\nmodels = {\n    'Random Forest': RandomForestClassifier(\n        n_estimators=200, max_depth=15, min_samples_split=5,\n        random_state=RANDOM_STATE, n_jobs=-1\n    ),\n    'Gradient Boosting': GradientBoostingClassifier(\n        n_estimators=100, learning_rate=0.1, max_depth=5,\n        random_state=RANDOM_STATE\n    ),\n    'Logistic Regression': LogisticRegression(\n        max_iter=1000, random_state=RANDOM_STATE\n    ),\n    'SVM': SVC(\n        kernel='rbf', C=1.0, probability=True,\n        random_state=RANDOM_STATE\n    )\n}\n\nresults = {}\ntrained_models = {}\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    model.fit(X_train_balanced, y_train_balanced)\n    trained_models[name] = model\n    \n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    \n    results[name] = {\n        'Accuracy': accuracy_score(y_test, y_pred),\n        'Precision': precision_score(y_test, y_pred, average='weighted'),\n        'Recall': recall_score(y_test, y_pred, average='weighted'),\n        'F1-Score': f1_score(y_test, y_pred, average='weighted'),\n        'ROC-AUC': roc_auc_score(y_test, y_pred_proba)\n    }\n    \n    print(f\"  \u2713 {name} complete\")\n\nprint(\"\\n\u2713 All models trained!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare model performance\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL PERFORMANCE COMPARISON\")\nprint(\"=\"*80)\n\nresults_df = pd.DataFrame(results).T.sort_values('F1-Score', ascending=False)\ndisplay(results_df.style.format('{:.4f}').background_gradient(cmap='RdYlGn', subset=['F1-Score', 'ROC-AUC']))\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\n# Bar plot\nresults_df.plot(kind='bar', ax=axes[0], alpha=0.8, edgecolor='black')\naxes[0].set_title('Model Performance Metrics', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Model')\naxes[0].set_ylabel('Score')\naxes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\naxes[0].legend(loc='lower right')\naxes[0].grid(axis='y', alpha=0.3)\naxes[0].set_ylim([0.5, 1.05])\n\n# Radar chart\ncategories = list(results_df.columns)\nangles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\nangles += angles[:1]\n\nax = plt.subplot(1, 2, 2, projection='polar')\ncolors = plt.cm.Set2(np.linspace(0, 1, len(results_df)))\n\nfor idx, (model_name, values) in enumerate(results_df.iterrows()):\n    values_list = values.tolist() + [values.tolist()[0]]\n    ax.plot(angles, values_list, 'o-', linewidth=2, label=model_name, color=colors[idx])\n    ax.fill(angles, values_list, alpha=0.15, color=colors[idx])\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(categories, size=10)\nax.set_ylim(0.5, 1.0)\nax.set_title('Model Performance Radar', fontsize=14, fontweight='bold', pad=20)\nax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\nax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Select best model\nbest_model_name = results_df['F1-Score'].idxmax()\nbest_model = trained_models[best_model_name]\n\nprint(f\"\\n\ud83c\udfc6 Best Model: {best_model_name}\")\nprint(f\"   F1-Score: {results_df.loc[best_model_name, 'F1-Score']:.4f}\")\nprint(f\"   ROC-AUC: {results_df.loc[best_model_name, 'ROC-AUC']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.5 Detailed Model Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed evaluation of best model\nprint(\"\\n\" + \"=\"*80)\nprint(f\"DETAILED EVALUATION: {best_model_name}\")\nprint(\"=\"*80)\n\ny_pred = best_model.predict(X_test_scaled)\ny_pred_proba = best_model.predict_proba(X_test_scaled)\n\n# Classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Count matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n            xticklabels=le.classes_, yticklabels=le.classes_)\naxes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\naxes[0].set_ylabel('Actual')\naxes[0].set_xlabel('Predicted')\n\n# Percentage matrix\ncm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\nsns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues', ax=axes[1],\n            xticklabels=le.classes_, yticklabels=le.classes_)\naxes[1].set_title('Confusion Matrix (%)', fontsize=14, fontweight='bold')\naxes[1].set_ylabel('Actual')\naxes[1].set_xlabel('Predicted')\n\nplt.tight_layout()\nplt.show()\n\n# Additional metrics\ntn, fp, fn, tp = cm.ravel()\nprint(\"\\nConfusion Matrix Breakdown:\")\nprint(f\"  True Negatives: {tn}\")\nprint(f\"  False Positives: {fp} (Healthy predicted as Stressed)\")\nprint(f\"  False Negatives: {fn} (Stressed predicted as Healthy)\")\nprint(f\"  True Positives: {tp}\")\nprint(f\"\\nSpecificity: {tn/(tn+fp):.4f}\")\nprint(f\"Sensitivity: {tp/(tp+fn):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.6 ROC Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ROC curves for all models\nprint(\"\\n\" + \"=\"*80)\nprint(\"ROC CURVE ANALYSIS\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# ROC curves\nfor name, model in trained_models.items():\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n    auc = roc_auc_score(y_test, y_pred_proba)\n    axes[0].plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={auc:.4f})')\n\naxes[0].plot([0,1], [0,1], 'k--', linewidth=2, label='Random')\naxes[0].set_xlabel('False Positive Rate', fontsize=12)\naxes[0].set_ylabel('True Positive Rate', fontsize=12)\naxes[0].set_title('ROC Curves', fontsize=14, fontweight='bold')\naxes[0].legend(loc='lower right')\naxes[0].grid(True, alpha=0.3)\n\n# Precision-Recall curves\nfor name, model in trained_models.items():\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n    axes[1].plot(recall, precision, linewidth=2, label=name)\n\naxes[1].set_xlabel('Recall', fontsize=12)\naxes[1].set_ylabel('Precision', fontsize=12)\naxes[1].set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\naxes[1].legend(loc='lower left')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.7 Cross-Validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cross-validation\nprint(\"\\n\" + \"=\"*80)\nprint(\"CROSS-VALIDATION (5-Fold Stratified)\")\nprint(\"=\"*80)\n\ncv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\ncv_results = []\n\nfor name, model in trained_models.items():\n    scores = cross_val_score(model, X_train_balanced, y_train_balanced,\n                             cv=cv_strategy, scoring='f1_weighted', n_jobs=-1)\n    cv_results.append({\n        'Model': name,\n        'Mean_F1': scores.mean(),\n        'Std_F1': scores.std(),\n        'Min_F1': scores.min(),\n        'Max_F1': scores.max()\n    })\n    print(f\"\\n{name}:\")\n    print(f\"  Mean F1: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n    print(f\"  Range: [{scores.min():.4f}, {scores.max():.4f}]\")\n\ncv_df = pd.DataFrame(cv_results).sort_values('Mean_F1', ascending=False)\n\n# Visualize\nplt.figure(figsize=(10, 6))\nplt.errorbar(range(len(cv_df)), cv_df['Mean_F1'], yerr=cv_df['Std_F1'],\n             fmt='o', markersize=10, capsize=5, capthick=2, linewidth=2)\nplt.xticks(range(len(cv_df)), cv_df['Model'], rotation=45, ha='right')\nplt.ylabel('F1-Score')\nplt.title('Cross-Validation F1-Score with Std Dev', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 Cross-validation complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Task 3: Spatial Analysis & Visualization\n\n### 4.1 Generate Field-Level Predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate predictions for entire field\nprint(\"\\n\" + \"=\"*80)\nprint(\"FIELD-LEVEL STRESS PREDICTION\")\nprint(\"=\"*80)\n\n# Prepare full dataset\nX_full = df[selected_features].copy()\nX_full_scaled = scaler.transform(X_full)\nX_full_scaled = pd.DataFrame(X_full_scaled, columns=selected_features, index=X_full.index)\n\n# Predictions\ndf['predicted_label'] = best_model.predict(X_full_scaled)\ndf['predicted_proba_stressed'] = best_model.predict_proba(X_full_scaled)[:, 1]\ndf['predicted_class'] = df['predicted_label'].map({0: 'Healthy', 1: 'Stressed'})\n\nprint(f\"\u2713 Predictions generated for {len(df)} observations\")\nprint(\"\\nPrediction Summary:\")\nprint(df['predicted_class'].value_counts())\nprint(f\"\\nStressed areas: {df['predicted_label'].mean()*100:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Create Stress Heatmaps"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive heatmaps\nprint(\"\\n\" + \"=\"*80)\nprint(\"FIELD STRESS HEATMAP VISUALIZATION\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(2, 2, figsize=(18, 16))\n\n# 1. Binary classification\nheatmap_binary = df.pivot_table(index='grid_y', columns='grid_x', \n                                values='predicted_label', aggfunc='mean')\nsns.heatmap(heatmap_binary, cmap='RdYlGn_r', ax=axes[0,0],\n            cbar_kws={'label': '0=Healthy, 1=Stressed'}, vmin=0, vmax=1)\naxes[0,0].set_title('Binary Crop Health Classification', fontsize=14, fontweight='bold')\naxes[0,0].set_xlabel('Grid X (East-West)')\naxes[0,0].set_ylabel('Grid Y (North-South)')\n\n# 2. Probability heatmap\nheatmap_proba = df.pivot_table(index='grid_y', columns='grid_x',\n                               values='predicted_proba_stressed', aggfunc='mean')\nsns.heatmap(heatmap_proba, cmap='RdYlGn_r', ax=axes[0,1],\n            cbar_kws={'label': 'Stress Probability'}, vmin=0, vmax=1)\naxes[0,1].set_title('Stress Probability Heatmap', fontsize=14, fontweight='bold')\naxes[0,1].set_xlabel('Grid X (East-West)')\naxes[0,1].set_ylabel('Grid Y (North-South)')\n\n# 3. NDVI distribution\nheatmap_ndvi = df.pivot_table(index='grid_y', columns='grid_x',\n                              values='ndvi_mean', aggfunc='mean')\nsns.heatmap(heatmap_ndvi, cmap='RdYlGn', ax=axes[1,0],\n            cbar_kws={'label': 'NDVI Value'})\naxes[1,0].set_title('NDVI Spatial Distribution', fontsize=14, fontweight='bold')\naxes[1,0].set_xlabel('Grid X (East-West)')\naxes[1,0].set_ylabel('Grid Y (North-South)')\n\n# 4. Moisture index\nheatmap_moisture = df.pivot_table(index='grid_y', columns='grid_x',\n                                 values='moisture_index', aggfunc='mean')\nsns.heatmap(heatmap_moisture, cmap='Blues', ax=axes[1,1],\n            cbar_kws={'label': 'Moisture Index'})\naxes[1,1].set_title('Moisture Index Spatial Distribution', fontsize=14, fontweight='bold')\naxes[1,1].set_xlabel('Grid X (East-West)')\naxes[1,1].set_ylabel('Grid Y (North-South)')\n\nplt.suptitle('Comprehensive Field Analysis Dashboard',\n             fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2713 Heatmaps generated!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 Identify Stress Zones"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Categorize stress severity\nprint(\"\\n\" + \"=\"*80)\nprint(\"STRESS ZONE IDENTIFICATION\")\nprint(\"=\"*80)\n\ndf['stress_severity'] = pd.cut(\n    df['predicted_proba_stressed'],\n    bins=[0, 0.3, 0.7, 1.0],\n    labels=['Low Risk', 'Moderate Risk', 'High Risk']\n)\n\nseverity_counts = df['stress_severity'].value_counts()\nprint(\"\\nStress Severity Distribution:\")\nfor severity, count in severity_counts.items():\n    pct = count/len(df)*100\n    print(f\"  {severity}: {count} cells ({pct:.1f}%)\")\n\n# Visualize stress zones\nfig, ax = plt.subplots(figsize=(14, 10))\n\ncolors = {'Low Risk': '#2ecc71', 'Moderate Risk': '#f39c12', 'High Risk': '#e74c3c'}\nfor severity in ['Low Risk', 'Moderate Risk', 'High Risk']:\n    mask = df['stress_severity'] == severity\n    ax.scatter(df[mask]['grid_x'], df[mask]['grid_y'],\n              c=colors[severity], label=severity, s=100, alpha=0.7,\n              edgecolors='black', linewidth=0.5)\n\nax.set_xlabel('Grid X (East-West)', fontsize=12)\nax.set_ylabel('Grid Y (North-South)', fontsize=12)\nax.set_title('Field Stress Severity Map', fontsize=16, fontweight='bold')\nax.legend(title='Stress Level', loc='upper right', fontsize=11)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.4 Grid-Level Aggregation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Grid statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"GRID-LEVEL AGGREGATION\")\nprint(\"=\"*80)\n\ngrid_stats = df.groupby(['grid_x', 'grid_y']).agg({\n    'predicted_proba_stressed': ['mean', 'std'],\n    'ndvi_mean': 'mean',\n    'moisture_index': 'mean',\n    'canopy_density': 'mean',\n    'predicted_label': 'mean'\n}).reset_index()\n\ngrid_stats.columns = ['grid_x', 'grid_y', 'stress_prob_mean', 'stress_prob_std',\n                      'ndvi_mean', 'moisture_mean', 'canopy_mean', 'stress_ratio']\n\nprint(\"\\nGrid Statistics Summary:\")\ndisplay(grid_stats.describe())\n\n# Critical zones\ncritical_zones = grid_stats[grid_stats['stress_prob_mean'] > 0.7].sort_values(\n    'stress_prob_mean', ascending=False\n)\n\nprint(f\"\\n\u26a0\ufe0f  Critical Zones (stress > 70%): {len(critical_zones)}\")\nif len(critical_zones) > 0:\n    print(\"\\nTop 10 Most Critical Cells:\")\n    display(critical_zones.head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Task 4: Drone & Agronomy Interpretation\n\n### 5.1 Actionable Insights"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Field analysis for drone operations\nprint(\"\\n\" + \"=\"*80)\nprint(\"DRONE INSPECTION STRATEGY - ACTIONABLE INSIGHTS\")\nprint(\"=\"*80)\n\ntotal_area = len(df)\nstressed_area = df['predicted_label'].sum()\nstress_pct = stressed_area / total_area * 100\n\nprint(\"\\n\ud83d\udcca Field Overview:\")\nprint(f\"   Total area: {total_area} grid cells\")\nprint(f\"   Stressed: {stressed_area} cells ({stress_pct:.1f}%)\")\nprint(f\"   Healthy: {total_area - stressed_area} cells ({100-stress_pct:.1f}%)\")\n\nhigh_risk = df[df['stress_severity'] == 'High Risk']\nmoderate_risk = df[df['stress_severity'] == 'Moderate Risk']\n\nprint(\"\\n\ud83c\udfaf Priority Zones:\")\nprint(f\"   High Risk: {len(high_risk)} cells (IMMEDIATE ACTION)\")\nprint(f\"   Moderate Risk: {len(moderate_risk)} cells (Monitor)\")\n\n# Identify stress causes\nstressed_df = df[df['predicted_label'] == 1]\nif len(stressed_df) > 0:\n    ndvi_low = (stressed_df['ndvi_mean'] < 0.5).sum() / len(stressed_df) * 100\n    moisture_low = (stressed_df['moisture_index'] < 0.5).sum() / len(stressed_df) * 100\n    canopy_low = (stressed_df['canopy_density'] < 0.6).sum() / len(stressed_df) * 100\n    \n    print(\"\\n\ud83d\udca1 Primary Stress Indicators:\")\n    print(f\"   Low NDVI (<0.5): {ndvi_low:.1f}%\")\n    print(f\"   Low Moisture (<0.5): {moisture_low:.1f}%\")\n    print(f\"   Low Canopy (<0.6): {canopy_low:.1f}%\")\n    \n    stress_factors = {\n        'Water Stress': moisture_low,\n        'Vegetation Vigor': ndvi_low,\n        'Sparse Coverage': canopy_low\n    }\n    primary = max(stress_factors, key=stress_factors.get)\n    print(f\"\\n   \ud83d\udd0d Primary factor: {primary}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 Comprehensive Drone Operation Plan"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate drone recommendations\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPREHENSIVE DRONE OPERATION PLAN\")\nprint(\"=\"*80)\n\nrecommendations = []\n\n# High priority zones\nif len(high_risk) > 0:\n    recommendations.append({\n        'Priority': '\ud83d\udd34 CRITICAL',\n        'Action': 'High-Resolution RGB Imaging',\n        'Target': f'{len(high_risk)} high-risk cells',\n        'Altitude': '20-30m',\n        'Timing': '24-48 hours',\n        'Purpose': 'Visual inspection & validation'\n    })\n\n# Moderate risk monitoring\nif len(moderate_risk) > 0:\n    recommendations.append({\n        'Priority': '\ud83d\udfe1 MEDIUM',\n        'Action': 'Multispectral Monitoring',\n        'Target': f'{len(moderate_risk)} moderate-risk cells',\n        'Altitude': '50-80m',\n        'Timing': 'Every 5-7 days',\n        'Purpose': 'Track stress progression'\n    })\n\n# Water stress\nif len(stressed_df) > 0 and moisture_low > 50:\n    recommendations.append({\n        'Priority': '\ud83d\udca7 IRRIGATION',\n        'Action': 'Thermal Imaging',\n        'Target': f'{int(len(stressed_df)*moisture_low/100)} cells',\n        'Altitude': '40-60m',\n        'Timing': 'Early morning (6-8 AM)',\n        'Purpose': 'Identify irrigation needs'\n    })\n\nrec_df = pd.DataFrame(recommendations)\nprint(\"\\n\ud83d\udccb Prioritized Action Plan:\")\ndisplay(rec_df)\n\nprint(\"\\n\u2708\ufe0f Flight Path Strategy:\")\nprint(\"   1. Start with high-risk clusters\")\nprint(\"   2. Progressive scan: North-South or East-West\")\nprint(\"   3. Maintain 70-80% overlap\")\nprint(\"   4. Adjust altitude by resolution needs\")\nprint(\"   5. Optimal lighting conditions\")\n\nprint(\"\\n\ud83d\udcc5 Monitoring Schedule:\")\nprint(\"   Week 1: Full field baseline\")\nprint(\"   Week 2: High-risk follow-up\")\nprint(\"   Week 3: Comparative analysis\")\nprint(\"   Week 4: Full rescan + effectiveness check\")\n\nif len(high_risk) > 0:\n    print(f\"\\n\ud83d\udccf Flight Requirements:\")\n    print(f\"   High-priority area: {len(high_risk)} cells\")\n    print(f\"   Estimated flight time: {len(high_risk)*0.5:.0f}-{len(high_risk)*1:.0f} min\")\n    print(f\"   Batteries needed: {int(np.ceil(len(high_risk)*0.5/20))}\")\n    print(f\"   Flights per day: 2-3 (morning/afternoon)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 Agronomic Recommendations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Agronomic interventions\nprint(\"\\n\" + \"=\"*80)\nprint(\"AGRONOMIC INTERVENTION RECOMMENDATIONS\")\nprint(\"=\"*80)\n\nprint(\"\\n\ud83c\udf3e Based on Analysis:\")\n\nif len(stressed_df) > 0:\n    if moisture_low > 60:\n        print(\"\\n1. IRRIGATION MANAGEMENT:\")\n        print(\"   - Increase irrigation frequency in identified zones\")\n        print(\"   - Consider drip/precision irrigation for targeted areas\")\n        print(\"   - Monitor soil moisture sensors\")\n        print(\"   - Schedule irrigation during cooler hours\")\n    \n    if ndvi_low > 50:\n        print(\"\\n2. NUTRIENT MANAGEMENT:\")\n        print(\"   - Conduct soil testing in low-NDVI zones\")\n        print(\"   - Apply targeted nitrogen fertilization\")\n        print(\"   - Consider foliar feeding for quick response\")\n        print(\"   - Monitor chlorophyll levels\")\n    \n    if canopy_low > 40:\n        print(\"\\n3. CROP MANAGEMENT:\")\n        print(\"   - Investigate pest/disease in sparse areas\")\n        print(\"   - Check for soil compaction issues\")\n        print(\"   - Evaluate planting density\")\n        print(\"   - Consider re-seeding if necessary\")\n\nprint(\"\\n4. CONTINUOUS MONITORING:\")\nprint(\"   - Weekly multispectral scans during critical growth\")\nprint(\"   - Bi-weekly thermal imaging\")\nprint(\"   - Ground-truth selected high-risk zones\")\nprint(\"   - Maintain detailed records for temporal analysis\")\n\nprint(\"\\n\u2713 Recommendations generated!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Task 5: Reflection & Future Improvements\n\n### 6.1 Project Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Project summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"PROJECT SUMMARY\")\nprint(\"=\"*80)\n\nprint(\"\\n\u2705 ACHIEVEMENTS:\")\nprint(f\"   1. Analyzed {len(df)} spatial observations\")\nprint(f\"   2. Trained {len(models)} ML models\")\nprint(f\"   3. Best model: {best_model_name}\")\nprint(f\"   4. F1-Score: {results_df.loc[best_model_name, 'F1-Score']:.4f}\")\nprint(f\"   5. Identified {len(high_risk)} critical zones\")\nprint(f\"   6. Generated actionable drone flight plans\")\n\nprint(\"\\n\ud83d\udcca KEY FINDINGS:\")\nprint(f\"   - {stress_pct:.1f}% of field shows stress\")\nprint(f\"   - Most important features: {', '.join(selected_features[:3])}\")\nif len(stressed_df) > 0:\n    print(f\"   - Primary stress factor: {primary}\")\nprint(f\"   - Model accuracy: {results_df.loc[best_model_name, 'Accuracy']:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.2 Limitations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Discuss limitations\nprint(\"\\n\" + \"=\"*80)\nprint(\"LIMITATIONS & CHALLENGES\")\nprint(\"=\"*80)\n\nprint(\"\\n\u26a0\ufe0f  IDENTIFIED LIMITATIONS:\")\n\nprint(\"\\n1. DATA LIMITATIONS:\")\nprint(\"   - Single-time snapshot (no temporal data)\")\nprint(\"   - No ground-truth validation data\")\nprint(\"   - Missing environmental factors (weather, soil)\")\nprint(\"   - Limited to processed indices (no raw imagery)\")\nprint(\"   - Unknown crop type and growth stage\")\n\nprint(\"\\n2. MODEL LIMITATIONS:\")\nprint(\"   - Binary classification (no stress severity levels)\")\nprint(\"   - Potential overfitting to this specific field\")\nprint(\"   - Limited feature engineering\")\nprint(\"   - No ensemble of best models\")\n\nprint(\"\\n3. OPERATIONAL LIMITATIONS:\")\nprint(\"   - Static analysis (not real-time)\")\nprint(\"   - No automated alert system\")\nprint(\"   - Requires manual intervention planning\")\nprint(\"   - No cost-benefit analysis\")\n\nprint(\"\\n4. VALIDATION CHALLENGES:\")\nprint(\"   - No field-verified stress labels\")\nprint(\"   - Cannot validate causal factors\")\nprint(\"   - Limited cross-field generalization testing\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.3 Future Improvements"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Propose improvements\nprint(\"\\n\" + \"=\"*80)\nprint(\"FUTURE IMPROVEMENTS & RECOMMENDATIONS\")\nprint(\"=\"*80)\n\nprint(\"\\n\ud83d\ude80 PROPOSED ENHANCEMENTS:\")\n\nprint(\"\\n1. DATA ENHANCEMENTS:\")\nprint(\"   \u2713 Temporal Analysis:\")\nprint(\"     - Multi-temporal scans (weekly/bi-weekly)\")\nprint(\"     - Time-series forecasting of stress progression\")\nprint(\"     - Seasonal pattern analysis\")\nprint(\"   \u2713 Additional Data Sources:\")\nprint(\"     - Weather data integration (rainfall, temperature)\")\nprint(\"     - Soil sensor data (moisture, pH, nutrients)\")\nprint(\"     - Historical yield data\")\nprint(\"     - Ground-truth validation samples\")\n\nprint(\"\\n2. MODEL IMPROVEMENTS:\")\nprint(\"   \u2713 Advanced Techniques:\")\nprint(\"     - Deep learning (CNN for raw imagery)\")\nprint(\"     - Multi-class classification (stress levels)\")\nprint(\"     - Regression models for stress intensity\")\nprint(\"     - Transfer learning from other fields\")\nprint(\"   \u2713 Feature Engineering:\")\nprint(\"     - Temporal features (rate of change)\")\nprint(\"     - Spatial features (neighboring cell patterns)\")\nprint(\"     - Interaction terms between indices\")\n\nprint(\"\\n3. OPERATIONAL ENHANCEMENTS:\")\nprint(\"   \u2713 Automation:\")\nprint(\"     - Real-time processing pipeline\")\nprint(\"     - Automated alert system\")\nprint(\"     - Dynamic flight path generation\")\nprint(\"     - Mobile app for field managers\")\nprint(\"   \u2713 Integration:\")\nprint(\"     - Farm management system integration\")\nprint(\"     - Variable rate application maps\")\nprint(\"     - ROI calculator for interventions\")\n\nprint(\"\\n4. VALIDATION & TESTING:\")\nprint(\"   \u2713 Ground Validation:\")\nprint(\"     - Field sampling protocol\")\nprint(\"     - Expert agronomist review\")\nprint(\"     - Yield correlation analysis\")\nprint(\"   \u2713 Cross-Validation:\")\nprint(\"     - Multiple fields/crops\")\nprint(\"     - Different regions/climates\")\nprint(\"     - Various growth stages\")\n\nprint(\"\\n5. EXPLAINABILITY:\")\nif SHAP_AVAILABLE:\n    print(\"   \u2713 SHAP analysis implemented\")\nelse:\n    print(\"   \u27a4 Implement SHAP for model interpretation\")\n\nif LIME_AVAILABLE:\n    print(\"   \u2713 LIME analysis implemented\")\nelse:\n    print(\"   \u27a4 Implement LIME for instance-level explanations\")\n    \nprint(\"   \u27a4 Create interactive dashboards\")\nprint(\"   \u27a4 Generate automated reports\")\nprint(\"   \u27a4 Develop decision support tools\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.4 Conclusions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final conclusions\nprint(\"\\n\" + \"=\"*80)\nprint(\"CONCLUSIONS\")\nprint(\"=\"*80)\n\nprint(\"\\n\ud83c\udfaf KEY TAKEAWAYS:\")\n\nprint(\"\\n1. TECHNICAL SUCCESS:\")\nprint(f\"   - Successfully developed ML pipeline achieving {results_df.loc[best_model_name, 'F1-Score']:.1%} F1-score\")\nprint(\"   - Identified spatial stress patterns effectively\")\nprint(\"   - Generated actionable insights for field management\")\n\nprint(\"\\n2. PRACTICAL VALUE:\")\nprint(\"   - Enables targeted interventions (cost savings)\")\nprint(\"   - Early stress detection (yield protection)\")\nprint(\"   - Data-driven decision making\")\nprint(f\"   - Prioritized {len(high_risk)} critical zones for immediate action\")\n\nprint(\"\\n3. AGRICULTURAL IMPACT:\")\nprint(\"   - Precision agriculture enabler\")\nprint(\"   - Resource optimization (water, fertilizer)\")\nprint(\"   - Sustainable farming practices\")\nprint(\"   - Scalable to large-scale operations\")\n\nprint(\"\\n4. NEXT STEPS:\")\nprint(\"   - Implement temporal monitoring system\")\nprint(\"   - Validate predictions with ground truth\")\nprint(\"   - Expand to multiple fields/crops\")\nprint(\"   - Integrate with farm management systems\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\u2713 CAPSTONE PROJECT COMPLETE\")\nprint(\"=\"*80)\nprint(\"\\nThis analysis demonstrates the power of combining:\")\nprint(\"  \u2022 Remote sensing (drone multispectral data)\")\nprint(\"  \u2022 Machine learning (AI classification)\")\nprint(\"  \u2022 Spatial analysis (geospatial visualization)\")\nprint(\"  \u2022 Domain expertise (agronomic interpretation)\")\nprint(\"\\nFor precision agriculture and sustainable crop management.\")\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Appendix: Additional Visualizations & Export\n\n### Export Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export predictions and statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPORTING RESULTS\")\nprint(\"=\"*80)\n\n# Export predictions\npredictions_export = df[['grid_x', 'grid_y', 'crop_health_label', \n                         'predicted_class', 'predicted_proba_stressed',\n                         'stress_severity', 'ndvi_mean', 'moisture_index']].copy()\n\npredictions_export.to_csv('crop_health_predictions.csv', index=False)\nprint(\"\u2713 Predictions exported: crop_health_predictions.csv\")\n\n# Export critical zones\nif len(critical_zones) > 0:\n    critical_zones.to_csv('critical_zones.csv', index=False)\n    print(\"\u2713 Critical zones exported: critical_zones.csv\")\n\n# Export model performance\nresults_df.to_csv('model_performance.csv')\nprint(\"\u2713 Model performance exported: model_performance.csv\")\n\nprint(\"\\n\u2713 All results exported successfully!\")\nprint(\"\\nFiles created:\")\nprint(\"  1. crop_health_predictions.csv - Full field predictions\")\nprint(\"  2. critical_zones.csv - High-risk zones for intervention\")\nprint(\"  3. model_performance.csv - ML model comparison\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## End of Analysis\n\n**Project completed successfully!**\n\nThis comprehensive analysis provides:\n- Deep understanding of crop health indicators\n- Robust machine learning models\n- Spatial stress mapping\n- Actionable drone operation plans\n- Critical reflection and future directions\n\n**For questions or additional analysis, please refer to the documentation above.**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}